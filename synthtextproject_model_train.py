# -*- coding: utf-8 -*-
"""SynthTextProject_model_train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18BxVOysx3boUuLmkJEMF-9P-50oUxFbN
"""

import numpy as np
from sklearn.model_selection import train_test_split
from keras.models import Sequential
from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout, PReLU
from tensorflow.keras.optimizers import Adam, SGD
import os
import h5py
import matplotlib.pyplot as plt
import cv2 as cv
import tensorflow as tf
from tensorflow.keras import regularizers
from math import sqrt
import random
import pickle
from sklearn import metrics
from sklearn.utils import shuffle

# These lines are meant for using the gpu, originally I worked on colab so I did not need them there, I added them just for you
## LIMIT GPU USAGE
config = tf.compat.v1.ConfigProto()
config.gpu_options.allow_growth = True  # don't pre-allocate memory; allocate as-needed
config.gpu_options.per_process_gpu_memory_fraction = 1  # limit memory to be allocated
K.set_session(tf.compat.v1.Session(config=config)) # create sess w/ above settings

size = 25 # resize the characters to (size,size)


# this is from the colab notebook to import
# from google.colab import drive
#
# folder = '/content/drive/MyDrive/SynthTextProject/'
# filePath = f'{folder}/SynthText.h5'
# drive.mount('/content/drive/')

filePath = 'SynthText.h5'

db = h5py.File(filePath, 'r')
im_names = list(db['data'].keys())

'''
  This function gets an image and returns a tuple (Images, Labels)
  Images = images of all the characters (cropped to the corresponding bounding box)
  Labels = the font of each character image 
'''
def get_characters_from_image(im):
  #get all data from the image - code from the powerPoint presentation
  img = db['data'][im][:]
  font = db['data'][im].attrs['font']
  txt = db['data'][im].attrs['txt']
  charBB = db['data'][im].attrs['charBB']
  wordBB = db['data'][im].attrs['wordBB']

  chars_images = []
  chars_labels = []
  for char in range(len(charBB[0,0,:])):
    # get the four corners' coordinates of the bounding box
    x1,y1 = charBB[0,0,char],charBB[1,0,char]
    x2,y2 = charBB[0,1,char],charBB[1,1,char]
    x3,y3 = charBB[0,2,char],charBB[1,2,char]
    x4,y4 = charBB[0,3,char],charBB[1,3,char]

    # get the min and max so we can crop them
    # basically get the bounding rectangle of the bounding box that is parallel to the axes
    x_min = int(min([x1,x2,x3,x4]))
    x_max = int(max([x1,x2,x3,x4]))

    y_min = int(min([y1,y2,y3,y4]))
    y_max = int(max([y1,y2,y3,y4]))

    # some coordinates are exceeding a little from the screen
    if y_min < 0:
      y_min = 0
    if x_min < 0:
      x_min = 0

    chars_images.append(img[y_min:y_max,x_min:x_max,:]) # crop the image
    chars_labels.append(font[char])
  return (chars_images, chars_labels)


# prepare all the data:
x = [] # characters images
y = [] # characters labels(fonts' names)

# extract characters from all images
for i in range(len(im_names)):
  chars_images, chars_fonts = get_characters_from_image(im_names[i])
  x.extend(chars_images)
  y.extend(chars_fonts)

# convert to numpy array
x = np.array(x)
y = np.array(y)

# convert every image to uint8 numpy array and scale them to same size
for i in range(len(x)):
  img = np.array(x[i], dtype='uint8')
  img = cv.resize(img,(size,size),interpolation=cv.INTER_CUBIC)
  shape = np.shape(img)
  x[i] = np.copy(img)

# make image random brightness and contrast using tensorflow data augmentation module
def randomize_image_brightness_contrast(img):
  im = np.copy(img)
  im = tf.image.random_brightness(im,0.3)
  im = tf.image.random_contrast(im,0.5,1.5)
  return im

# make image random hue using tensorflow data augmentation module
def randomize_image_hue(img):
  im = np.copy(img)
  im = tf.image.random_hue(im,0.5)
  return im

# convert the labels to indices of the labels array
labels = np.array(['Roboto','Ubuntu Mono','Russo One','Raleway','Michroma','Alex Brush', 'Open Sans'])

for i in range(len(y)):
  font = y[i].decode("utf-8") # convert from byte to string for tensorflow
  y[i] = np.where(font == labels)[0][0]
y = np.array(y)

# prepare the data for the neural network - a giagantic array of type float32 for the images' array and uint8 for the labels' array
x = np.stack(x, axis=0)
x = np.asarray(x).astype('float32')
x = x / 255
y = np.asarray(y).astype('uint8')

# split to train and test
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.15, shuffle=True)

#convert the train back to uint8 so we can augment it
x_train = x_train * 255
x_train = np.array(x_train, dtype='uint8')

# rotate an image and scale it, using zero padding
def rotate(im, angle, scale):
  img = np.copy(im)

  center = tuple(np.array(im.shape[1::-1]) / 2)
  rot_mat = cv.getRotationMatrix2D(center, angle, scale)
  img = cv.warpAffine(img, rot_mat, dsize=(im.shape[1],im.shape[0]), borderMode=cv.BORDER_CONSTANT)
  return img

# augment the data:
#   rotate and scale
#   random brightness and contrast
#   random hue

x_augment = []
y_augment = []
for i in range(len(x_train)):
  # rotate the images

  # degrees
  r1 = cv.rotate(x_train[i], cv.ROTATE_90_CLOCKWISE)
  r2 = cv.rotate(x_train[i], cv.ROTATE_90_COUNTERCLOCKWISE)
  
  # 45 degrees
  r3 = rotate(x_train[i],45, 0.8)
  r4 = rotate(x_train[i],-45, 0.8)

  # 27 degrees
  r5 = rotate(x_train[i],27, 0.7)
  r6 = rotate(x_train[i],-27, 0.7)

  #randomize the unrotated image as well
  bc = randomize_image_brightness_contrast(x_train[i])
  h = randomize_image_hue(x_train[i])

  #add them all up
  x_augment.extend([r1,r2,bc,h,r3,r4,r5,r6])
  y_augment.extend([y_train[i],y_train[i],y_train[i],y_train[i],y_train[i],y_train[i],y_train[i],y_train[i]])

# convert to augmented data to numpy float32 array
x_augment = np.array(x_augment, dtype='float32')
y_augment = np.array(y_augment)

x_train = np.stack(x_train, axis=0)
x_augment = np.stack(x_augment, axis=0)

# make one giant train set with the original and the augmented train data
x_train = np.concatenate((x_train, x_augment), axis=0)
y_train = np.concatenate((y_train, y_augment), axis=0)

x_train = np.stack(x_train, axis=0)

# normalize x_train between 0 and 1 again
x_train = np.array(x_train, dtype='float32')
for i in range(len(x_train)):
  x_train[i] = x_train[i] / 255

# shuffle the data
x_train,y_train=shuffle(x_train,y_train)

# create a validation set of the first 500 images on the test data to analyze performance during training

val_x = x_test[:500]
val_y = y_test[:500]
val_set = (val_x, val_y)

# create the neural network architecture:
# consists of 5 convolutional layers with different filter sizes and quantities

model = tf.keras.models.Sequential([
    tf.keras.layers.Conv2D(512, (7,7), activation='relu', padding='same', input_shape=(size,size,3), kernel_regularizer=regularizers.l1(1e-5)),
    tf.keras.layers.Dropout(0.2),

    tf.keras.layers.Conv2D(512, (7,7), activation='relu', padding='same', kernel_regularizer=regularizers.l1(1e-5)),
    tf.keras.layers.Dropout(0.2),

    tf.keras.layers.Conv2D(512, (7,7), activation='relu', padding='same', kernel_regularizer=regularizers.l1(1e-5)),

    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.MaxPooling2D((2,2), 2),
    tf.keras.layers.Dropout(0.25),

    tf.keras.layers.Conv2D(1024, (5,5), activation='relu', padding='same', kernel_regularizer=regularizers.l1(1e-5)),
    tf.keras.layers.Dropout(0.2),

    tf.keras.layers.Conv2D(1024, (3,3), activation='relu', padding='same', kernel_regularizer=regularizers.l1(1e-5)),

    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.MaxPooling2D((2,2),2),
    tf.keras.layers.Dropout(0.25),

    tf.keras.layers.Flatten(),

    tf.keras.layers.Dropout(0.5),

    tf.keras.layers.Dense(4096, activation='relu', kernel_regularizer=regularizers.l1(1e-5)),
    tf.keras.layers.Dropout(0.5),
    
    tf.keras.layers.Dense(7, activation='softmax')
])

# use the adam optimizer with custom learning rate and amsgrad
opt = Adam(0.001/3, amsgrad=True)

model.compile(loss = 'sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])

# fit using 100 epochs, 128 batch size, with the first 500 test samples as validation set
training = model.fit(x_train, y_train, validation_data=val_set, epochs=50*2, batch_size=4*32, verbose=1)

# plot train/validation accuracy to epoch graph after training is completed
plt.plot(training.history['accuracy'])
plt.plot(training.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

# evaluate the results of the model
print(model.evaluate(x_test,y_test))

# get classification report on each class
print(f'{metrics.classification_report(y_test,np.argmax(model.predict(x_test), axis=1), target_names=labels)}')

# save as h5
model.save(f'model_h5_.h5')

# this is to save it as pickle h5 was better
# # # uncomment to save the model as pickle
# # with open(f'model_pickle', 'wb') as f:
# #   pickle.dump(model,f)
#
# # load model from memory
# with open(f'model_pickle_80', 'rb') as f:
#   mp = pickle.load(f)

# load as h5
mp = tf.keras.models.load_model('model_h5_.h5')

# evaluate it
print(mp.evaluate(x_test,y_test))

# print the architecture of the network
print(mp.summary())